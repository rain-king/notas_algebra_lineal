\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}

\title{Notas de Algebra Lineal}
\author{Murillo Vega, Gustavo \\ e-mail:
\href{mailto:g.murillo24@info.uas.edu.mx}{g.murillo24@info.uas.edu.mx}}
\date{October 2024}

\begin{document}

\maketitle

\section{Introducción}
\subsection{Definición de matriz}
Sea ${F}$ el campo de los reales o de los complejos, definimos una matriz $A\in {F}^{m\times n}$ por sus propiedades de 1 a 4.
\begin{enumerate}
    \item Denotamos sus elementos por $A_{i,j}\in A$ con  $i=1,\ldots,m$ y $j=1,\ldots,n$. También, podemos denotar una matriz por sus elementos:
    
    $(a_{i,j})_{m\times n} \in {F}^{m\times n}$, donde redundantemente $((a_{i,j})_{m\times n})_{i,j} := a_{i,j}$.\\
    Cuando no haya confusión con las dimensiones de la matriz
    escribimos\\ $(a_{i,j}) := (a_{i,j})_{m\times n}$.
    
    \item Definimos la suma de matrices $A+B = C$ para
    \\ $A, B \in {F}^{m\times n}$ como:
    $$C_{i,j}=A_{i,j} + B_{i,j}.$$

    \item Se define el producto $cA$ de $A$ por el escalar $c$
    como $$(cA)_{i,j} = cA_{i,j}.$$
    
    \item Como se verá en seguida, es útil definir el producto
    $AB\in {F}^{m\times n}$ de las matrices $A\in {F}^{m\times d}$ y
    $B \in {F}^{d\times n}$ de la siguiente manera:
    $$AB_{i,j} = \sum^d_{k=1} A_{i,k} B_{k,j}.$$
    
    Nota: Si las dimensiones de $A$ y $B$ son $m\times d$
    y $d\times n$, el producto $AB$ tiene dimensiones $m\times n$

    \item Si $A\in F^{m\times n}$, definimos $A^t \in F^{n\times m}$
    como $A^t_{i,j} = A_{j,i}$. Y $A^*$ como
    $A^*_{i,j} = \overline{A^t}$.

    \item \textit{Notación de arreglo:} Si
    $A\in {F}^{m\times n}$, escribimos
    $$A = \begin{bmatrix}
        a_{1,1} & a_{1,2} & \ldots & a_{1, n} \\
        a_{2,1} & a_{2,2} & \ldots & a_{2, n} \\
        \vdots & \vdots & \vdots & \vdots \\
        a_{m,1} & a_{m,2} & \ldots & a_{m, n}
    \end{bmatrix}$$
    si $A_{i,j} = a_{i,j}$.
\end{enumerate}

\subsection{Identificación de vectores columna y escalares}
Un una matriz $x$ de dimensión $m\times 1$, en el contexto de matrices, se dice que $x$ es un vector $m$-dimensional en $F^{m\times 1}$, y denotamos $x_i := x_{i,j}$
pues $j$ siempre es $1$. También identificamos
$F^{m\times 1}$ con ${F}^m$. Por lo tanto,
al hablar de vectores en el contexto de matrices, un vector
es una matriz que consiste de una sola columna.

Sea $\alpha\in F^{1\times 1}$, identificamos $\alpha_{i,j}$ con
$\alpha$ y $F$ con $F^{1\times 1}$, es decir identificamos $\alpha$
como un escalar.

\subsection{Ecuaciones lineales en forma de matriz}
El sistema de ecuaciones lineales general es:
\begin{align*}
    \sum^n_{j=1}a_{1,j} x_j &= b_1 \\
    \sum^n_{j=1}a_{2,j} x_j &= b_2 \\
    &\vdots \\
    \sum^n_{j=1}a_{m,j} x_j &= b_m.
\end{align*}

Ahora, veamos la forma de la multiplicación de $A=(a_{i,j})\in F^{m\times n}$
por $x=(x_1, x_2, \ldots, x_n)^t \in F^{n\times 1}$:
$$Ax = 
\begin{bmatrix}
    a_{1,1} & a_{1,2} & \ldots & a_{1, n} \\
    a_{2,1} & a_{2,2} & \ldots & a_{2, n} \\
    \vdots & \vdots & \vdots & \vdots \\
    a_{m,1} & a_{m,2} & \ldots & a_{m, n}
\end{bmatrix}
\begin{bmatrix}
    x_1 \\
    x_2 \\
    \vdots \\
    x_n
\end{bmatrix}
=
\begin{bmatrix}
    \sum^n_{j=1} a_{1,j} x_j \\ 
    \sum^n_{j=1} a_{2,j} x_j \\
    \vdots \\
    \sum^n_{j=1} a_{m,j} x_j \\ 
\end{bmatrix}.$$
Si inspeccionamos esta expresión con la del sistema de ecuaciones lineales, tenemos con $b=(b_1, b_2, \ldots, b_m)^t$, que el mismo sistema de ecuaciones se puede expresar como
$$Ax = b.$$

\subsection{Algunos productos usados más adelante}
\textbf{Producto interno}

Si $x$ y $y$ son vectores de la misma dimensión, definimos su
producto interior $(x,y)$ como
\[(x,y) = x^t \overline y\]
o bien
\[(x,y) = y^* x\]
\\
\textbf{Producto exterior}

Si $x=(x_1,\ldots,x_n)^t$ y $y=(y_1,\ldots, y_n)^t$, entonces
su producto exterior es
$$x y* =
\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}
\begin{bmatrix}
    \overline{y_1} & \ldots & \overline{y_n}
\end{bmatrix} =
\begin{bmatrix}
    x_1 \overline {y_1} & \ldots & x_1 \overline {y_n} \\
    x_2 \overline{y_2} & \ldots & x_2 \overline{y_n} \\
    \vdots & \vdots & \vdots \\
    x_n \overline{y_1} & \ldots & x_n \overline{y_n}
\end{bmatrix}
$$
\newpage

\section{Espacios Vectoriales}
(Usamos la palabra mapeo y transformación de forma intercambiable.)

Ya que vimos que los sistemas de ecuaciones lineales se pueden expresar con matrices, es de interés saber qué tipo de matrices generan sistemas de ecuaciones con soluciones, solución única o ninguna solución, para esto necesitamos la herramienta de mapeos lineales, que serán análogos a la función que corresponde a la multiplicación por la izquierda de una matriz con un vector columna. Sin embargo, estas son funciones de un espacio vectorial a otro, por lo que primero tenemos que definir qué es un espacio vectorial.

Como nota adicional, diremos que puede ser largamente innecesario para los practicantes de ciencias e ingeniería considerar mapeos sobre espacios vectoriales abstractos, en lugar de las transformaciones dadas por matrices sobre los vectores columna. Esto porque como se puede demostrar, tales ambas transformaciones son equivalentes por medio de isomorfismos.

Sin embargo para desarrollar teoría del álgebra lineal, e incluso modelos dentro de la ciencia y la ingeniería, es prolifero considerar estas transformaciones y espacios vectoriales abstractamente.

\subsection{Definición de espacio vectorial}
Un espacio vectorial $V(F)$ sobre un campo $F$, también denotado $V$ cuando el campo es implicado, tiene elementos que cumplen las siguientes propiedades:
\begin{enumerate}
    \item \textbf{Suma y multiplicación escalar}
    
    Si $x,y\in V(F)$ y $\alpha \in F$, existe $x+\alpha y \in V$.
    Donde el operador $+$ es conmutativo y asociativo.
    
    \item \textbf{Existencia del 0}
    
    Existe $0\in V$ tal que si $x\in V$, $x+0 = x$.

    \item \textbf{Identidad escalar}
    
    $1x = x$ para $x\in V$, por virtud de $0+1x$.

    \item \textbf{Existencia del inverso aditivo}

    Si $x\in V$ existe $-x\in V$ tal que $x+(-x) = 0$.
    
    \item \textbf{Asociatividad de vector con escalar}

    Si $\alpha, \beta\in F$ y $x\in V$,
    $(\alpha\beta)x = \alpha (\beta x)$.
    
    \item \textbf{Distributividad}

    Si $\alpha,\beta\in F$ y $x,y\in V$
    \begin{itemize}
        \item $(\alpha+\beta)x = \alpha x + \beta x$.
        \item $\alpha (x + y) = \alpha x + \alpha y$.
    \end{itemize}
\end{enumerate}

Los elementos de un espacio vectorial se llaman vectores.

\subsubsection*{Ejemplos}
Esto implica que el conjunto de matrices $F^{m\times n}$ con sus
operaciones (excluyendo la multiplicación) son un espacio vectorial $V(F)$.

En particular $F^{m\times 1}$ es un espacio vectorial, de donde
en ingeniería o ciencias a estos elementos son a los que generalmente se llaman vectores. Notemos la diferencia, que aunque estos son vectores en el sentido del espacio vectorial también son matrices columna (o vectores en el sentido matricial). Pero también, las matrices son vectores en su propio espacio vectorial.

El conjunto de funciones $f: X \rightarrow F$ de un conjunto $X$
a un campo $F$ forman un campo vectorial. En particular, los polinomios $P^n(t)$ sobre un campo $F$ y $t$ en un anillo $R$ con identidad (ejemplo, las matrices con todas sus operaciones, o algún campo) son un espacio vectorial. Para aclarar, un polinomio de tal espacio tiene
la forma $\sum^n_{i=0} \alpha_i t^n$ con $\alpha_i\in F$.
\newpage

\section{Bases y dimensiones}
\subsection{Combinación lineal}
Una combinación lineal de vectores $x_i \in V$ es el vector $\sum_i \alpha_i x_i$ con $\alpha_i\in F$.

\newcommand{\spn}{\text{span }}
\subsection{Conjunto generador y espacio generado}
Sea $U\subset V$, se dice que $U$ es el conjunto generador de $W$,
o equivalentemente que $W$ es generado por $U$, si 
$W$ es el conjunto de todas las combinaciones lineales de $U$.
Denotamos tal $W$ por $\spn U$.

Nótese que $\spn U\subset V$ y es un espacio vectorial.

\subsection{Subespacio vectorial}
Un subespacio vectorial $U$ de $V$ es un espacio vectorial
con el mismo campo y operaciones que $V$, tal que $U\subset V$.

\subsubsection*{Ejemplo}
$\spn U$ donde $U\subset V$ es subespacio de $V$. Y $V$ es subespacio de $V$.

\subsection{Independencia lineal}
Un conjunto de vectores $x_i$ es independiente linealmente si
cuando una combinación lineal de ellos es $0$, todos los coeficientes
correspondientes $\alpha_i$ son igual a $0$. Es decir
$$\sum_i \alpha_i x_i \implies \alpha_i = 0.$$

\subsubsection*{Ejemplo}
El conjunto que consiste de $0$ no es linealmente independiente, pues
$\alpha 0 = 0$ no implica que $\alpha = 0$.

El conjunto de un elemento distinto de $0$, $v$, es linealmente independiente.

\subsection{Base}
Se dice que $B\subset V$ es una base para $V$, si
$\spn B = V$ y los vectores en $B$ son linealmente
independientes.

\subsubsection*{Ejemplo}
El conjunto $O=\{0\}$ genera al espacio vectorial $O(F)$
que consiste de $0$. Sin embargo $O$, como ya vimos, no es linealmente independiente,
por lo tanto no es base para $O(F)$.
Excluimos el caso en que $F$ consiste de $0$, pues entonces $O(F)$ sí es linealmente independiente.
Por supuesto, tal campo no lo tenemos en consideración. Comúnmente por $F$ nos referimos
a los complejos o los reales, y en raras ocasiones (no en estas notas)
a campos finitos o contables.

\subsection{Dimensión}
La dimensión de $V$ es la cardinalidad de una base de $V$.
Esto está bien definido pues se puede probar que todas las bases de
$V$ tienen la misma cardinalidad. 

En el caso de que la base sea
finita, la dimensión es el número de vectores en la base. Pero
también hay espacios vectoriales con bases numerables, 
donde las combinaciones lineales en general son una serie.

Como ya hemos dicho antes, el conjunto que consiste de $0$ no es base, sin embargo es el único conjunto que genera a $V$ que consiste de $0$. Así que por convención decimos que $V$ tiene dimensión $0$.
\newpage

\section{Mapeos lineales}
\subsection{Motivación}
Nótese que la función $f: F^n \rightarrow F^m$ dada por
$f(x) = Ax$ donde $A\in F^{m\times n}$, tiene las propiedades
\begin{enumerate}
    \item \textbf{Aditividad}

    $f(x+y) = f(x) + f(y)$ para $x,y\in F^n$.

    \item \textbf{Homogenidad}

    $f(\alpha x) = \alpha f(x)$ para $\alpha\in F$.
\end{enumerate}
Generalizamos estas funciones de la siguiente manera.

\subsection{Mapeo lineal}
Una función $T: V\rightarrow W$ con $V,W$ de espacios vectoriales es
un mapeo lineal si cumple con las dos condiciones anteriores, reemplazando $V$ por $F^n$ y $W$ por $F^m$. Usualmente,
denotamos $T(x)$ por $Tx$ para $x\in V$.

\subsection{Espacio de mapeos lineales}
Denotamos por $\mathcal{L}(V,W)$ al conjunto de todas
las transformaciones lineales de $V$ a $W$.
Y denotamos $\mathcal{L}(V)$ a $\mathcal{L}(V, V)$. A estos últimos
les llamamos \textit{operadores lineales}.

Se puede demostrar que $\mathcal{L}(V,W)$ es un espacio vectorial.

%\newcommand{\ker}{\text{ker }}
\subsection{Kernel}
Decimos que el espacio nulo, o kernel, de $T: V \rightarrow W$, es el conjunto de $x\in V$ tales que $Tx=0$. Y lo denotamos por
$\ker T$.

Podemos probar que $\ker T$ es un subespacio de $V$:
Sea $x \in \spn \ker T$, es decir $x = \sum_i \alpha_i x_i$ donde
$x_i\in \ker T$, entonces $Tx = \sum_i \alpha_i Tx_i = 0$ pues
cada $Tx_i=0$, sigue que $x\in\ker T$.

\subsubsection*{Inyectividad}
En general, una función $f$ es inyectiva si $x\neq y$ implica
$f(x) \neq f(y)$.

En particular, podemos probar que un mapeo lineal $T: V\rightarrow W$
es inyectivo si $\ker T = \{0\}$: Sean $x\neq y$ vectores de $V$,
de manera que $x-y\neq 0$, y supóngase que $\ker T$ consiste de $0$. Luego $T(x-y) \neq 0$, pues de lo contrario implicaría $x-y = 0$,
de manera que $T(x-y) = Tx - Ty \neq 0$ o $Tx \neq Ty$, y $T$ es
inyectiva.

El regreso también es verdadero: Sea $T: V\rightarrow W$ inyectiva,
la contrapuesta de la definición de inyectividad nos dice que
dados $x,y\in V$
$Tx = Ty$ implica $x = y$, esto es $T(x-y) = 0$ implica $x-y = 0$.
Luego $\ker T$ consiste de $0$.

\textbf{Resultado importante:}
El mapeo lineal $T$ es inyectivo ssi $\dim\ker T = 0$.

\newcommand{\im}{\text{im }}
\subsection{Imagen}
Dado $T: V\rightarrow W$, la imagen de $T$ es
$T(V) = \{Tx: x\in V\}$. Y la denotamos por $\im T$.

Se puede demostrar, que análogo a $\ker T$, $\im T$ forma un
subespacio de $W$: Sea $y\in \spn\im T$, entonces
$y=\sum_i \beta_i y_i$ donde $y_i=Tx_i$ para algún $x_i\in V$.
Luego $y=T(\sum_i \beta_i x_i) = Tx$ para
$x=\sum_i \beta_i x_i \in V$; $y\in\im T$.

\subsubsection*{Sobreyectividad}
Trivialmente, $T: V\rightarrow W$ es sobreyectiva si $\im T = W$.

\subsection{Importancia del kernel y la imagen}
Si tenemos el sistema de ecuaciones lineales homogéneo, $Ax=0$,
es de interés saber si hay otras soluciones además de $x=0$.
Esto se puede responder si encontramos que el mapeo $T_A: F^n \rightarrow F^m$
dada por $T_A(x) = Ax$, tiene kernel distinto de $\{0\}$.

Si tenemos un sistema de ecuaciones lineales $Ax=b$, es de
interés si existen soluciones para $x$, y si existe alguna,
si es única. Esto se podrá responder con teoremas a partir de propiedades tanto la imagen como del kernel de la transformación $T_A$.

\subsection{Teorema fundamental de los mapeos lineales}
Sea $T: V\rightarrow W$ un mapeo lineal de dimensión finita, entonces
$$\dim V = \dim\ker T + \dim\im T.$$

\subsubsection*{Prueba}
Formen $u_i$ con $i$ de $1$ hasta $n$ una base para $\ker T$,
al ser subespacio de $V$, esta base se puede extender a una base $B$
de $V$. Donde $B$ consiste de los $n$ $u_i$, y
$v_j$ con $j$ de $1$ hasta $m$. De manera que la dimensión de $V$
es $\dim\ker T+m$.

Ahora consideremos $x\in V$, expandido de la base $B$, es decir\\
$x= \sum^n_{i=1}\alpha_iu_i + \sum^m_{j=1}\beta_jv_j$. Luego
$Tx = \sum^m_{j=1}\beta_jTv_j$. Queremos mostrar que $Tv_j$ de
$1$ hasta $m$ forma una base para $\im T$. Primero, como $x$ era arbitrario, $\spn \{Tv_j\}_j = \im T$.

Sea $\sum^m_{j=1}\beta_jTv_j = T(\sum_j\beta_jv_j)=0$.
Entonces $\sum_j\beta_jv_j \in \ker T$, y escrito respecto de la base
de los $u_i$,
$$\sum_j\beta_jv_j = \sum_i\alpha_iu_i$$
luego
$$\sum_j\beta_jv_j + \sum_i(-\alpha_i)u_i = 0$$
pero como los $v_j$ con los $u_i$ forman una base para $V$,
son linealmente independientes y $\beta_j=\alpha_i=0$.

Hemos probado que $\sum^m_{j=1}\beta_jTv_j = 0$ implica
$\beta_j = 0$, es decir, la independencia lineal de los $Tv_j$.
\hfill ////

\subsubsection{Corolario}
Un mapeo lineal $T:V\rightarrow W$ a un espacio de menor dimensión no es inyectivo.
\subsubsection*{Prueba}
\begin{align*}
\dim\ker T &= \dim V - \dim\im T\\
    &\geq \dim V - \dim W\\
    &> 0
\end{align*}
Como probamos antes, $T$ es inyectiva solo si $\dim \ker T = 0$,
por lo tanto no es inyectivo.\hfill ////

\subsubsection{Corolario}
Un mapeo lineal $T: V\rightarrow W$ a un espacio de dimensión mayor
no es sobreyectivo.

\subsubsection*{Prueba}
\begin{align*}
    \dim\im T &= \dim V - \dim\ker T\\
    &< \dim W - \dim\ker T \\
    &\leq \dim W
\end{align*}
Entonces $\im T \neq W$, es decir, $T$ no es sobreyectivo.
\hfill ////

\subsection{Aplicaciones a sistemas de ecuaciones lineales}
Supongamos que $Ax=b$ tiene una solución. Entonces podemos saber
que la solución es única, si $T_A$ es invertible (inyectiva y sobre), de manera que $T_A^{-1}b = x$ es la única solución.

Se puede probar que si $T_A$ no es invertible, la solución no es única. Por lo cual, usando los dos corolarios anteriores, basta que $m\neq n$ para que la solución no sea única. 

Estos dos temas se desarrollarán más adelante en invertibilidad de mapeos lineales.

\subsubsection{Proposición}
Si $Ax=b$ tiene más ecuaciones que variables, existen $b$ para
los cuales el sistema no tiene solución.

Esto sigue directamente del segundo corolario al teorema fundamental de los mapeos lineales: Si $A\in F^{m\times n}$ con $m>n$, esto implica que $T_A: F^n\rightarrow F^m$ mapea hacía un espacio de mayor dimensión, por lo que no es sobreyectivo. Es decir, existen $b$ para
los cuales no hay $x$ tal que $Ax=b$.
\newpage

\section{Matrices y mapeos lineales}
Hemos definido ad hoc la transformación relacionada a la matriz $A\in F^{m\times n}$.
En particular, no usamos bases para el dominio y contradominio de tal transformación.
Ahora veremos con más detenimiento la relación de las matrices con las transformaciones lineales,
y las bases de los espacios dominio y contradominio.

\subsection{Renglones y columnas}
La matriz renglón $i$ de la matriz $A$ la denotamos
$A_{i,.}$, y la columna $j$ por $A_{.,j}$.

\subsection{La matriz de un mapeo lineal}
Dado un mapeo lineal $T: V\rightarrow W$ y bases $B=\{b_i\}^n_{i=1}$
$C=\{c_j\}^m_{j=1}$, definimos la matriz $A\in F^{m\times n}$ tal que
$$Tb_j = \sum^m_{i=1} A_{i,j} c_i$$
para $j=1,\ldots,n$.

Dicho de otra forma, los elementos de $A_{.,j}$ son los coeficientes
de $Tb_j$ expandido en la base $C$ de $W$. Y denotamos a $A$, por
$\mathcal{M}(T,B,C)$, o cuando no haya confusión con las bases,
$\mathcal{M}(T)$.

\subsection{Ejemplo}
Haremos notar, que encontrar la matriz correspondiente a un mapeo lineal
es un cuanto confuso, pues al expandir un vector del contradominio,
tenemos una suma donde los coeficientes de la columna se encuentran
en forma horizontal, sugiriendo erróneamente que corresponden a un
renglón.

Sea $T:F^3\rightarrow F^2$ dada con las bases estándar para el dominio y
contradominio por $T(x,y,z)^t = (x,y + z)^t$, tenemos:
{
$$T\begin{pmatrix}
    1\\
    0\\
    0    
\end{pmatrix}
 = 
 1\begin{pmatrix}
    1\\
    0
 \end{pmatrix}
 +0\begin{pmatrix}
    0\\
    1
 \end{pmatrix}
 $$
 $$T\begin{pmatrix}
    0\\
    1\\
    0
 \end{pmatrix}
 =
 0\begin{pmatrix}
    1\\
    0
 \end{pmatrix}
 +1\begin{pmatrix}
    0\\
    1
 \end{pmatrix}
 $$
 $$
 T\begin{pmatrix}
    0\\
    0\\
    1
 \end{pmatrix}
 =
 0\begin{pmatrix}
    1\\
    0
 \end{pmatrix}
 +
 1\begin{pmatrix}
    0\\
    1
 \end{pmatrix}
 $$
}
De manera que
$$\mathcal{M}(T) = \begin{bmatrix}
    1 & 0 & 0\\
    0 & 1 & 1
\end{bmatrix}$$

\subsection{Teoremas de multiplicación de matrices}
\subsubsection*{Nota}
Con la notación anterior de renglones y columnas, podemos
escribir el elemento $(i,j)$ de $AB$ como
$$A_{i,.} B_{.,j}$$

\subsubsection{Columna o renglón de una multiplicación}
La columna $j$ de la multiplicación de $AB$ es igual a la
multiplicación de $A$ por la columna $j$ de $B$. Es decir
$(AB)_{.,j} = A B_{.,j}$. Análogamente se tiene lo mismo para renglones,
$(AB)_{i,.} = A_{i,.} B$.

\subsubsection*{Prueba}
El elemento $i$ de $(AB)_{.,j}$ es $A_{i,.} B_{.,j}$.
El elemento $i$ de $AB_{.,j}$ es igualmente, $A_{i,.}B_{.,j}$.
Análogamente para renglones. \hfill ////

\subsubsection{La multiplicación de una matriz por un vector (columna)}
Sea $A\in F^{m\times n}$ y $x=(x_1,\ldots,x_n)^t\in F^{n\times 1}$,
entonces
$$Ax=\sum^n_{j=1}A_{.,j} x_j$$
Es decir, $Ax$ es la combinación lineal de las columnas de $A$
con los elementos de $x$ como coeficientes.
\subsubsection*{Prueba}
El elemento $i$ de $Ax$ es
$$A_{i,.}x=\sum^n_{j=1}A_{i,j}x_j$$
mientras que el elemento $i$ de $\sum^n_{j=1}A_{.,j} x_j$
es claramente también
$$\sum^n_{j=1}A_{i,j} x_j$$
\hfill ////

\subsubsection{Multiplicación de matrices como combinación lineal
de columnas o renglones}
\begin{enumerate}
    \item La columna $j$ de $AB$ es la combinación lineal de columnas
    de $A$ con los elementos de la columna $j$ de $B$ como coeficientes.
    \item El renglón $i$ de $AB$ es la combinación lineal de renglones
    de $B$ con los elementos del renglón $i$ de $A$ como coeficientes.
\end{enumerate}
\subsubsection*{Prueba}
1. $(AB)_{.,j}$ es $AB_{.,j}$ como ya probamos, a su vez esto
es un producto de matriz por columna, entonces es la combinación lineal
de columnas de $A$ con los elementos de la columna $B_{.,j}$ como
coeficientes.

2. $(AB)_{i,.}$ es $A_{i,.}B$. Su transpuesta es $B^t{A_{i,.}}^t$, donde
${A_{i,.}}^t$ es una columna, por lo tanto $B^t{A_{i,.}}^t$ es la combinación lineal de columnas de $B^t$ con
elementos de ${A_{i,.}}^t$ como coeficientes. La transpuesta de la
transpuesta, es el renglón original $(AB)_{i,.}$. Entonces
$(AB)_{i,.}$ es la combinación lineal de renglones (que eran columnas en $B^t$) de coeficientes del renglón $A_{i,.}$.
\hfill ////

\subsection{Factorización columna-renglón y rango de una matriz}
\subsubsection{Rango de columnas y renglones}
El rango de las columnas de una matriz $A\in F^{m\times n}$
es la dimensión de \\
$\spn \{A_{.,j}\}_{j=1}^n$ es decir, del
espacio generado por las columnas de $A$.

Análogamente se define el rango de renglones de $A$ como la 
dimensión del espacio generado por los renglones de $A$.

\subsubsection{Factorización columna-renglón}
Sea $A\in F^{m\times n}$ con rango de columnas $p\geq 1$,
entonces existen $C\in F^{m\times p}$ y $R\in F^{p\times n}$
tal que $A=CR$.
\subsubsection*{Prueba}
Sea $C$ la matriz cuyas columnas corresponden a una base del
espacio de columnas de $A$, como el rango de columnas de $A$ es $p$,
$C$ es $m\times p$.

Cada columna $j$ de $A$ es una combinación lineal
de columnas de $C$ pues estas son básicas para el espacio de columnas
de $A$. Sean los coeficientes de tal combinación lineal, los elementos
de las columna $j$ de la matriz $R$, entonces $R$ es $p\times n$.

Por sus dimensiones existe $CR$ de dimensión $m\times n$, cuya columna
$j$ es $CR_{.,j}$, luego esta columna es combinación lineal de columnas
de $C$ con coeficientes de la columna $j$ de $R$, que como dijimos, son
exactamente los coeficientes para que tal combinación lineal sea la columna $j$ de $A$. \hfill ////
\end{document}
