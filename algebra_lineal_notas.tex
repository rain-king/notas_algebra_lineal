\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}

\title{Notas de Algebra Lineal}
\author{Murillo Vega, Gustavo \\ e-mail:
\href{mailto:g.murillo24@info.uas.edu.mx}{g.murillo24@info.uas.edu.mx}}
\date{Octubre 2024}

\begin{document}

\maketitle

\section{Introducción}
\subsection{Definición de matriz}
Sea ${F}$ el campo de los reales o de los complejos, definimos una matriz $A\in {F}^{m\times n}$ por los puntos 1 a 4.
\begin{enumerate}
    \item Denotamos sus elementos por $A_{i,j}\in A$ con  $i=1,\ldots,m$ y $j=1,\ldots,n$. También, podemos denotar una matriz por sus elementos:
    
    $(a_{i,j})_{m\times n} \in {F}^{m\times n}$, donde redundantemente $((a_{i,j})_{m\times n})_{i,j} := a_{i,j}$.\\
    Cuando no haya confusión con las dimensiones de la matriz
    escribimos\\ $(a_{i,j}) := (a_{i,j})_{m\times n}$.
    
    \item Definimos la suma de matrices $A+B = C$ para
    \\ $A, B \in {F}^{m\times n}$ como:
    $$C_{i,j}=A_{i,j} + B_{i,j}.$$

    \item Se define el producto $cA$ de $A$ por el escalar $c$
    como $$(cA)_{i,j} = cA_{i,j}.$$
    
    \item Como se verá en seguida, es útil definir el producto
    $AB\in {F}^{m\times n}$ de las matrices $A\in {F}^{m\times d}$ y
    $B \in {F}^{d\times n}$ de la siguiente manera:
    $$AB_{i,j} = \sum^d_{k=1} A_{i,k} B_{k,j}.$$
    
    Nota: Si las dimensiones de $A$ y $B$ son $m\times d$
    y $d\times n$, el producto $AB$ tiene dimensiones $m\times n$

    \item Si $A\in F^{m\times n}$, definimos la transpuesta $A^t$
    de $A$ por ${A^t}_{i,j} = A_{j,i}$.
    Y la adjunta de $A$ como $A^* = \overline{A^t}$.

    \item \textit{Notación de arreglo:} Si
    $A\in {F}^{m\times n}$, escribimos
    $$A = \begin{bmatrix}
        A_{1,1} & A_{1,2} & \ldots & A_{1, n} \\
        A_{2,1} & A_{2,2} & \ldots & A_{2, n} \\
        \vdots & \vdots & \vdots & \vdots \\
        A_{m,1} & A_{m,2} & \ldots & A_{m, n}
    \end{bmatrix}$$
\end{enumerate}

\subsection{Identificación de vectores columna y escalares}
Un una matriz $x$ de dimensión $m\times 1$, en el contexto de matrices, se dice que $x$ es un vector $m$-dimensional en $F^{m\times 1}$, y denotamos $x_i := x_{i,j}$
pues $j$ siempre es $1$. También identificamos
$F^{m\times 1}$ con ${F}^m$. Por lo tanto,
al hablar de vectores en el contexto de matrices, un vector
es una matriz que consiste de una sola columna.

Sea $\alpha\in F^{1\times 1}$, identificamos $\alpha_{i,j}$ con
$\alpha$ y $F$ con $F^{1\times 1}$, es decir identificamos $\alpha$
como un escalar.

\subsection{Ecuaciones lineales en forma de matriz}
El sistema de ecuaciones lineales general es:
\begin{align*}
    \sum^n_{j=1}a_{1,j} x_j &= b_1 \\
    \sum^n_{j=1}a_{2,j} x_j &= b_2 \\
    &\vdots \\
    \sum^n_{j=1}a_{m,j} x_j &= b_m.
\end{align*}

Ahora, veamos la forma de la multiplicación de $A=(a_{i,j})\in F^{m\times n}$
por $x=(x_1, x_2, \ldots, x_n)^t \in F^{n\times 1}$:
$$Ax = 
\begin{bmatrix}
    a_{1,1} & a_{1,2} & \ldots & a_{1, n} \\
    a_{2,1} & a_{2,2} & \ldots & a_{2, n} \\
    \vdots & \vdots & \vdots & \vdots \\
    a_{m,1} & a_{m,2} & \ldots & a_{m, n}
\end{bmatrix}
\begin{bmatrix}
    x_1 \\
    x_2 \\
    \vdots \\
    x_n
\end{bmatrix}
=
\begin{bmatrix}
    \sum^n_{j=1} a_{1,j} x_j \\ 
    \sum^n_{j=1} a_{2,j} x_j \\
    \vdots \\
    \sum^n_{j=1} a_{m,j} x_j \\ 
\end{bmatrix}.$$
Si inspeccionamos esta expresión con la del sistema de ecuaciones lineales, tenemos con $b=(b_1, b_2, \ldots, b_m)^t$, que el mismo sistema de ecuaciones se puede expresar como
$$Ax = b.$$

\subsection{Algunos productos usados más adelante}
\textbf{Producto interno}

Si $x$ y $y$ son vectores de la misma dimensión, definimos su
producto interior $(x,y)$ como
\[(x,y) = x^t \overline y\]
o bien
\[(x,y) = y^* x\]
\\
\textbf{Producto exterior}

Si $x=(x_1,\ldots,x_n)^t$ y $y=(y_1,\ldots, y_n)^t$, entonces
su producto exterior es
$$x y* =
\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}
\begin{bmatrix}
    \overline{y_1} & \ldots & \overline{y_n}
\end{bmatrix} =
\begin{bmatrix}
    x_1 \overline {y_1} & \ldots & x_1 \overline {y_n} \\
    x_2 \overline{y_2} & \ldots & x_2 \overline{y_n} \\
    \vdots & \vdots & \vdots \\
    x_n \overline{y_1} & \ldots & x_n \overline{y_n}
\end{bmatrix}
$$
\newpage

\section{Espacios Vectoriales}
(Usamos la palabra mapeo y transformación de forma intercambiable.)

Ya que vimos que los sistemas de ecuaciones lineales se pueden expresar con matrices, es de interés saber qué tipo de matrices generan sistemas de ecuaciones con soluciones, solución única o ninguna solución, para esto necesitamos la herramienta de mapeos lineales, que serán análogos a la función que corresponde a la multiplicación por la izquierda de una matriz con un vector columna. Sin embargo, estas son funciones de un espacio vectorial a otro, por lo que primero tenemos que definir qué es un espacio vectorial.

Como nota adicional, diremos que puede ser largamente innecesario para los practicantes de ciencias e ingeniería considerar mapeos sobre espacios vectoriales abstractos, en lugar de las transformaciones dadas por matrices sobre los vectores columna. Esto porque como se puede demostrar, tales ambas transformaciones son equivalentes por medio de isomorfismos.

Sin embargo para desarrollar teoría del álgebra lineal, e incluso modelos dentro de la ciencia y la ingeniería, es prolifero considerar estas transformaciones y espacios vectoriales abstractamente.

\subsection{Definición de espacio vectorial}
Un espacio vectorial $V(F)$ sobre un campo $F$, también denotado $V$ cuando el campo es implicado, tiene elementos que cumplen las siguientes propiedades:
\begin{enumerate}
    \item \textbf{Suma y multiplicación escalar}
    
    Si $x,y\in V(F)$ y $\alpha \in F$, existe $x+\alpha y \in V$.
    Donde el operador $+$ es conmutativo y asociativo.
    
    \item \textbf{Existencia del 0}
    
    Existe $0\in V$ tal que si $x\in V$, $x+0 = x$.

    \item \textbf{Identidad escalar}
    
    $1x = x$ para $x\in V$, por virtud de $0+1x$.

    \item \textbf{Existencia del inverso aditivo}

    Si $x\in V$ existe $-x\in V$ tal que $x+(-x) = 0$.
    
    \item \textbf{Asociatividad de vector con escalar}

    Si $\alpha, \beta\in F$ y $x\in V$,
    $(\alpha\beta)x = \alpha (\beta x)$.
    
    \item \textbf{Distributividad}

    Si $\alpha,\beta\in F$ y $x,y\in V$
    \begin{itemize}
        \item $(\alpha+\beta)x = \alpha x + \beta x$.
        \item $\alpha (x + y) = \alpha x + \alpha y$.
    \end{itemize}
\end{enumerate}

Los elementos de un espacio vectorial se llaman vectores.

\subsubsection*{Ejemplos}
Esto implica que el conjunto de matrices $F^{m\times n}$ con sus
operaciones (excluyendo la multiplicación) son un espacio vectorial $V(F)$.

En particular $F^{m\times 1}$ es un espacio vectorial, de donde
en ingeniería o ciencias a estos elementos son a los que generalmente se llaman vectores. Notemos la diferencia, que aunque estos son vectores en el sentido del espacio vectorial también son matrices columna (o vectores en el sentido matricial). Pero también, las matrices son vectores en su propio espacio vectorial.

El conjunto de funciones $f: X \rightarrow F$ de un conjunto $X$
a un campo $F$ forman un campo vectorial. En particular, los polinomios $P^n(t)$ sobre un campo $F$ y $t$ en un anillo $R$ con identidad (ejemplo, las matrices con todas sus operaciones, o algún campo) son un espacio vectorial. Para aclarar, un polinomio de tal espacio tiene
la forma $\sum^n_{i=1} \alpha_i t^n$ con $\alpha_i\in F$.
\newpage

\section{Bases y dimensiones}
\subsection{Combinación lineal}
Una combinación lineal de vectores $x_i \in V$ es el vector $\sum_i \alpha_i x_i$ con $\alpha_i\in F$.

\newcommand{\spn}{\text{span }}
\subsection{Conjunto generador y espacio generado}
Sea $U\subset V$, se dice que $U$ es el conjunto generador de $W$,
o equivalentemente que $W$ es generado por $U$, si 
$W$ es el conjunto de todas las combinaciones lineales de $U$.
Denotamos tal $W$ por $\spn U$.

Nótese que $\spn U\subset V$ y es un espacio vectorial.

\subsection{Subespacio vectorial}
Un subespacio vectorial $U$ de $V$ es un espacio vectorial
con el mismo campo y operaciones que $V$, tal que $U\subset V$.

\subsubsection*{Ejemplo}
$\spn U$ donde $U\subset V$ es subespacio de $V$. Y $V$ es subespacio de $V$.

\subsection{Independencia lineal}
Un conjunto de vectores $x_i$ es independiente linealmente si
cuando una combinación lineal de ellos es $0$, todos los coeficientes
correspondientes $\alpha_i$ son igual a $0$. Es decir
$$\sum_i \alpha_i x_i \implies \alpha_i = 0.$$

\subsubsection*{Ejemplo}
El conjunto que consiste de $0$ no es linealmente independiente, pues
$\alpha 0 = 0$ no implica que $\alpha = 0$.

El conjunto de un elemento distinto de $0$, $v$, es linealmente independiente.

\subsection{Base}
Se dice que $B\subset V$ es una base para $V$, si
$\spn B = V$ y los vectores en $B$ son linealmente
independientes.

\subsubsection*{Ejemplo}
El conjunto $O=\{0\}$ genera al espacio vectorial $O(F)$
que consiste de $0$. Sin embargo $O$, como ya vimos, no es linealmente independiente,
por lo tanto no es base para $O(F)$.
Excluimos el caso en que $F$ consiste de $0$, pues entonces $O(F)$ sí es linealmente independiente.
Por supuesto, tal campo no lo tenemos en consideración. Comúnmente por $F$ nos referimos
a los complejos o los reales, y en raras ocasiones (no en estas notas)
a campos finitos o contables.

\subsection{Dimensión}
La dimensión de $V$ es la cardinalidad de una base de $V$.
Esto está bien definido pues se puede probar que todas las bases de
$V$ tienen la misma cardinalidad. 

En el caso de que la base sea
finita, la dimensión es el número de vectores en la base. Pero
también hay espacios vectoriales con bases numerables, 
donde las combinaciones lineales en general son una serie.

Como ya hemos dicho antes, el conjunto que consiste de $0$ no es base, sin embargo es el único conjunto que genera a $V$ que consiste de $0$. Así que por convención decimos que $V$ tiene dimensión $0$.
\newpage

\section{Mapeos lineales}
\subsection{Motivación}
Nótese que la función $f: F^n \rightarrow F^m$ dada por
$f(x) = Ax$ donde $A\in F^{m\times n}$, tiene las propiedades
\begin{enumerate}
    \item \textbf{Aditividad}

    $f(x+y) = f(x) + f(y)$ para $x,y\in F^n$.

    \item \textbf{Homogenidad}

    $f(\alpha x) = \alpha f(x)$ para $\alpha\in F$.
\end{enumerate}
Generalizamos estas funciones de la siguiente manera.

\subsection{Mapeo lineal}
Una función $T: V\rightarrow W$ con $V,W$ de espacios vectoriales es
un mapeo lineal si cumple con las dos condiciones anteriores, reemplazando $V$ por $F^n$ y $W$ por $F^m$. Usualmente,
denotamos $T(x)$ por $Tx$ para $x\in V$.

\subsection{Espacio de mapeos lineales}
Denotamos por $\mathcal{L}(V,W)$ al conjunto de todas
las transformaciones lineales de $V$ a $W$.
Y denotamos $\mathcal{L}(V)$ a $\mathcal{L}(V, V)$. A estos últimos
les llamamos \textit{operadores lineales}. En particular el operador
$I\in\mathcal{L}(V)$ definido por $Ix=x$ se llama la identidad en $V$.

Se puede demostrar fácilmente que $\mathcal{L}(V,W)$ es un espacio vectorial.

%\newcommand{\ker}{\text{ker }}
\subsection{Kernel}
Decimos que el espacio nulo, o kernel, de $T: V \rightarrow W$, es el conjunto de $x\in V$ tales que $Tx=0$. Y lo denotamos por
$\ker T$.

Podemos probar que $\ker T$ es un subespacio de $V$:
Sea $x \in \spn \ker T$, es decir $x = \sum_i \alpha_i x_i$ donde
$x_i\in \ker T$, entonces $Tx = \sum_i \alpha_i Tx_i = 0$ pues
cada $Tx_i=0$, sigue que $x\in\ker T$.

\subsubsection*{Inyectividad}
En general, una función $f$ es inyectiva si $x\neq y$ implica
$f(x) \neq f(y)$.

En particular, podemos probar que un mapeo lineal $T: V\rightarrow W$
es inyectivo si $\ker T = \{0\}$: Sean $x\neq y$ vectores de $V$,
de manera que $x-y\neq 0$, y supóngase que $\ker T$ consiste de $0$. Luego $T(x-y) \neq 0$, pues de lo contrario implicaría $x-y = 0$,
de manera que $T(x-y) = Tx - Ty \neq 0$ o $Tx \neq Ty$, y $T$ es
inyectiva.

El regreso también es verdadero: Sea $T: V\rightarrow W$ inyectiva,
la contrapuesta de la definición de inyectividad nos dice que
dados $x,y\in V$
$Tx = Ty$ implica $x = y$, esto es $T(x-y) = 0$ implica $x-y = 0$.
Luego $\ker T$ consiste de $0$.

\textbf{Resultado importante:}
El mapeo lineal $T$ es inyectivo ssi $\dim\ker T = 0$.

\newcommand{\im}{\text{im }}
\subsection{Imagen}
Dado $T: V\rightarrow W$, la imagen de $T$ es
$T(V) = \{Tx: x\in V\}$. Y la denotamos por $\im T$.

Se puede demostrar, que análogo a $\ker T$, $\im T$ forma un
subespacio de $W$: Sea $y\in \spn\im T$, entonces
$y=\sum_i \beta_i y_i$ donde $y_i=Tx_i$ para algún $x_i\in V$.
Luego $y=T(\sum_i \beta_i x_i) = Tx$ para
$x=\sum_i \beta_i x_i \in V$; $y\in\im T$.

\subsubsection*{Sobreyectividad}
Trivialmente, $T: V\rightarrow W$ es sobreyectiva si $\im T = W$.

\subsection{Importancia del kernel y la imagen}
Si tenemos el sistema de ecuaciones lineales homogéneo, $Ax=0$,
es de interés saber si hay otras soluciones además de $x=0$.
Esto se puede responder si encontramos que el mapeo $T_A: F^n \rightarrow F^m$
dada por $T_A(x) = Ax$, tiene kernel distinto de $\{0\}$.

Si tenemos un sistema de ecuaciones lineales $Ax=b$, es de
interés si existen soluciones para $x$, y si existe alguna,
si es única. Esto se podrá responder con teoremas a partir de propiedades tanto la imagen como del kernel de la transformación $T_A$.

\subsection{Teorema fundamental de los mapeos lineales}
Sea $T: V\rightarrow W$ un mapeo lineal con $V$ de dimensión finita,
entonces
$$\dim V = \dim\ker T + \dim\im T.$$

\subsubsection*{Prueba}
Formen $u_i$ con $i$ de $1$ hasta $n$ una base para $\ker T$,
al ser subespacio de $V$, esta base se puede extender a una base $B$
de $V$. Donde $B$ consiste de los $n$ $u_i$, y
$v_j$ con $j$ de $1$ hasta $m$. De manera que la dimensión de $V$
es $\dim\ker T+m$.

Ahora consideremos $x\in V$, expandido de la base $B$, es decir\\
$x= \sum^n_{i=1}\alpha_iu_i + \sum^m_{j=1}\beta_jv_j$. Luego
$Tx = \sum^m_{j=1}\beta_jTv_j$. Queremos mostrar que $Tv_j$ de
$1$ hasta $m$ forma una base para $\im T$. Primero, como $x$ era arbitrario, $\spn \{Tv_j\}_j = \im T$.

Sea $\sum^m_{j=1}\beta_jTv_j = T(\sum_j\beta_jv_j)=0$.
Entonces $\sum_j\beta_jv_j \in \ker T$, y escrito respecto de la base
de los $u_i$,
$$\sum_j\beta_jv_j = \sum_i\alpha_iu_i$$
luego
$$\sum_j\beta_jv_j + \sum_i(-\alpha_i)u_i = 0$$
pero como los $v_j$ con los $u_i$ forman una base para $V$,
son linealmente independientes y $\beta_j=\alpha_i=0$.

Hemos probado que $\sum^m_{j=1}\beta_jTv_j = 0$ implica
$\beta_j = 0$, es decir, la independencia lineal de los $Tv_j$.
\hfill ////

\subsubsection{Corolario}
Un mapeo lineal $T:V\rightarrow W$ a un espacio de menor dimensión no es inyectivo.
\subsubsection*{Prueba}
\begin{align*}
\dim\ker T &= \dim V - \dim\im T\\
    &\geq \dim V - \dim W\\
    &> 0
\end{align*}
Como probamos antes, $T$ es inyectiva solo si $\dim \ker T = 0$,
por lo tanto no es inyectivo.\hfill ////

\subsubsection{Corolario}
Un mapeo lineal $T: V\rightarrow W$ a un espacio de dimensión mayor
no es sobreyectivo.

\subsubsection*{Prueba}
\begin{align*}
    \dim\im T &= \dim V - \dim\ker T\\
    &< \dim W - \dim\ker T \\
    &\leq \dim W
\end{align*}
Entonces $\im T \neq W$, es decir, $T$ no es sobreyectivo.
\hfill ////

\subsection{Invertibilidad}
Una función invertible es una función inyectiva y sobreyectiva.
Se demuestra en cursos de álgebra superior que esta definición es
equivalente a que para $f:A\rightarrow B$ exista una función
$g:B\rightarrow A$ tal que $(f\circ g)(y) = y$ para $y\in B$,
y $(g\circ f)(x) = x$ para $x\in A$.

\subsubsection{Invertibilidad en espacios de la misma dimensión}
Sean $\dim V = \dim W < \infty$.
\begin{enumerate}
    \item Si $T\in\mathcal{L}(V,W)$ entonces
    inyectividad, invertibilidad y sobreyectividad son equivalentes.
    \item Si $T\in\mathcal{L}(V,W)$ y $S\in\mathcal{L}(W,V)$,
    $ST=I$ o $TS=I$, implica la invertibilidad de ambos mapeos.
\end{enumerate}
\subsubsection*{Prueba}
\begin{enumerate}
    \item Si $T$ es inyectiva, $\ker T$ tiene dimensión $0$, de manera 
    que\\
    $\dim V = \dim \im T$, por hipótesis $\dim V = \dim W$ y
    $\dim \im T = \dim W$, es decir, $T$ es sobreyectiva, y por lo tanto
    invertible.
    
    Si $T$ es sobreyectiva, $\im T = W$, de manera que
    $$\dim V = \dim \ker T + \dim W \implies \dim \ker T = 0$$
    Es decir, $T$ es inyectiva y por lo tanto invertible.

    \item Sea $ST=I$, y $v\in\ker T$, entonces
    $$v = (ST)v = S(Tv) = S0 = 0$$
    por lo que $T$ es inyectiva al ser $\ker T$ de dimensión $0$,
    y por lo tanto invertible.

    Multiplicando $ST$ por $T^{-1}$ por la derecha llegamos a que,
    $S=T^{-1}$, y por ser la inversa de otra función, es invertible. 

    Análogamente si $TS=I$ se sigue el mismo procedimiento con
    $w\in\ker S$, y se llega a que $S$ es invertible. Luego a que $T$ 
    es invertible. \hfill ////
\end{enumerate}

\subsubsection{La inversa de la multiplicación}
Sean $T\in\mathcal{L}(V,W)$ y $S\in\mathcal{L}(W,V)$ invertibles, 
entonces $ST$ es invertible y $(ST)^{-1}=T^{-1}S^{-1}$.
\subsubsection*{Prueba}
Existen $S^{-1}$ y $T^{-1}$, entonces
\begin{align*}
    ST(T^{-1}S^{-1}) &= S(TT^{-1})S^{-1}\\
    &= SIS^{-1}\\
    &= SS^{-1} = I
\end{align*}
\hfill ////

\subsubsection{Espacios vectoriales isomorfos}
Se dice que un espacio vectorial $V$ es isomorfo a otro $W$,
si existe un mapeo lineal invertible $T$ de $V$ a $W$. Usualmente en 
este contexto, a $T$ se le llama isomorfismo de $V$ a $W$.

\subsubsection{La dimensión indica isomorfidad}
Dos espacios de dimensión finita son isomorfos
si y sólo si son de la misma dimensión.
\subsubsection*{Prueba}
Sea $T\in\mathcal{L}(V,W)$ un isomorfismo, porque es invertible es 
inyectivo y sobreyectivo, es decir $\dim\ker T = 0$ y
$\dim\im T = \dim W$, por el teorema fundamental de mapeos lineales
sigue $$\dim V = \dim W$$

Ahora si $\dim V = \dim W$, al ser de la misma dimensión, sea $n$, las 
bases de $V$ y $W$ se pueden escribir como $v_i$ y $w_i$ 
respectivamente con $i$ de $1$ a $n$. Entonces existe un 
$T\in\mathcal{L}(V,W)$ definido por
$$T\left(\sum^n_{i=1}c_i v_i\right) = \sum^n_{i=1}c_i w_i$$
con $c_i\in F$. Este $T$ es claramente sobreyectivo pues $Tx$ es una
expansión de la base del contradominio. Entonces es invertible, y
$V$ isomorfo a $W$. \hfill ////

\newpage

\section{Matrices}
En la introducción mostramos algunas propiedades básicas de las
matrices, y su capacidad de expresar sistemas de ecuaciones lineales.
A continuación mostraremos más propiedades importantes de matrices.

\subsection{Renglones y columnas}
La matriz renglón $i$ de la matriz $A$ la denotamos
$A_{i,.}$, y la columna $j$ por $A_{.,j}$.

\subsection{Teoremas de multiplicación de matrices}
\subsubsection*{Nota}
Con la notación anterior de renglones y columnas, podemos
escribir el elemento $(i,j)$ de $AB$ como
$$A_{i,.} B_{.,j}$$

\subsubsection{Columna o renglón de una multiplicación}
La columna $j$ de la multiplicación de $AB$ es igual a la
multiplicación de $A$ por la columna $j$ de $B$. Es decir
$(AB)_{.,j} = A B_{.,j}$. Análogamente se tiene lo mismo para renglones,
$(AB)_{i,.} = A_{i,.} B$.

\subsubsection*{Prueba}
El elemento $i$ de $(AB)_{.,j}$ es $A_{i,.} B_{.,j}$.
El elemento $i$ de $AB_{.,j}$ es igualmente, $A_{i,.}B_{.,j}$.
Análogamente para renglones. \hfill ////

\subsubsection{La multiplicación de una matriz por un vector (columna)}
Sea $A\in F^{m\times n}$ y $x=(x_1,\ldots,x_n)^t\in F^{n\times 1}$,
entonces
$$Ax=\sum^n_{j=1}A_{.,j} x_j$$
Es decir, $Ax$ es la combinación lineal de las columnas de $A$
con los elementos de $x$ como coeficientes.
\subsubsection*{Prueba}
El elemento $i$ de $Ax$ es
$$A_{i,.}x=\sum^n_{j=1}A_{i,j}x_j$$
mientras que el elemento $i$ de $\sum^n_{j=1}A_{.,j} x_j$
es claramente también
$$\sum^n_{j=1}A_{i,j} x_j$$
\hfill ////

\subsubsection{Multiplicación de matrices como combinación lineal
de columnas o renglones}
\begin{enumerate}
    \item La columna $j$ de $AB$ es la combinación lineal de columnas
    de $A$ con los elementos de la columna $j$ de $B$ como coeficientes.
    \item El renglón $i$ de $AB$ es la combinación lineal de renglones
    de $B$ con los elementos del renglón $i$ de $A$ como coeficientes.
\end{enumerate}
\subsubsection*{Prueba}
1. $(AB)_{.,j}$ es $AB_{.,j}$ como ya probamos, a su vez esto
es un producto de matriz por columna, entonces es la combinación lineal
de columnas de $A$ con los elementos de la columna $B_{.,j}$ como
coeficientes.

2. $(AB)_{i,.}$ es $A_{i,.}B$. Su transpuesta es $B^t{A_{i,.}}^t$, donde
${A_{i,.}}^t$ es una columna, por lo tanto $B^t{A_{i,.}}^t$ es la combinación lineal de columnas de $B^t$ con
elementos de ${A_{i,.}}^t$ como coeficientes. La transpuesta de la
transpuesta, es el renglón original $(AB)_{i,.}$. Entonces
$(AB)_{i,.}$ es la combinación lineal de renglones de $B$ (cuyas transpuestas eran las columnas en $B^t$) con coeficientes del renglón $A_{i,.}$.
\hfill ////

\subsection{Factorización columna-renglón y rango de una matriz}
\subsubsection{Rango de columnas y renglones}
El rango de las columnas de una matriz $A\in F^{m\times n}$
es la dimensión de \\
$\spn \{A_{.,j}\}_{j=1}^n$ es decir, del
espacio generado por las columnas de $A$.

Análogamente se define el rango de renglones de $A$ como la 
dimensión del espacio generado por los renglones de $A$.

\subsubsection{Factorización columna-renglón}
Sea $A\in F^{m\times n}$ con rango de columnas $p\geq 1$,
entonces existen $C\in F^{m\times p}$ y $R\in F^{p\times n}$
tal que $A=CR$.
\subsubsection*{Prueba}
Sea $C$ la matriz cuyas columnas corresponden a una base del
espacio de columnas de $A$, como el rango de columnas de $A$ es $p$,
$C$ es $m\times p$.

Cada columna $j$ de $A$ es una combinación lineal
de columnas de $C$ pues estas son básicas para el espacio de columnas
de $A$. Sean los coeficientes de tal combinación lineal, los elementos
de las columna $j$ de la matriz $R$, entonces $R$ es $p\times n$.

Por sus dimensiones existe $CR$ de dimensión $m\times n$, cuya columna
$j$ es $CR_{.,j}$, luego esta columna es combinación lineal de columnas
de $C$ con coeficientes de la columna $j$ de $R$, que como dijimos, son
exactamente los coeficientes para que tal combinación lineal sea la columna $j$ de $A$. \hfill ////

\subsubsection{Rango de columnas es el rango de renglones}
Si $A\in F^{m\times n}$, su rango de columnas es igual a su rango
de renglones.
\subsubsection*{Prueba}
Sea $p\geq 1$ el rango de columnas de $A$, y factorizamos $A$
por columna-renglón $CR$. Como cada renglón de $A$ es
combinación lineal de los renglones de $R$, y $R$ tiene $p$
renglones, el rango de las columnas de $A$ es a lo más $p$.
Es decir, el rango de renglones es menor o igual que el rango de 
columnas.

De la misma manera si $q\geq 1$ es el rango de renglones, en $A^t$
este es su rango de columnas. Y por el argumento anterior, su
rango de renglones es menor o igual que su rango de columnas.
En la matriz original, esto es que el rango de columnas es menor
o igual que el rango de renglones. Probando para el caso que el rango 
de columnas y el rango de renglones es al menos $1$, estos son
iguales.

Si el rango de columnas es $0$, este es el caso en que el espacio de
columnas no tiene base, pero es generado por $\{0\}$, de manera que
todas las columnas son $0$, y el rango de renglones también es $0$. Análogamente cuando el rango de renglones es $0$.
\hfill ////

\subsubsection{Rango}
Finalmente, podemos definir el rango de una matriz como el rango de
columnas, pues este es igual al rango de renglones. 

\section{Relación de matrices y mapeos lineales}
\subsection{Matriz de un mapeo lineal}
Dado un mapeo lineal $T: V\rightarrow W$ y bases $B=\{b_i\}^n_{i=1}$
$C=\{c_j\}^m_{j=1}$, definimos la matriz $A\in F^{m\times n}$ tal que
$$Tb_j = \sum^m_{i=1} A_{i,j} c_i$$
para $j=1,\ldots,n$.

\newcommand{\M}{\mathcal{M}}
Dicho de otra forma, los elementos de $A_{.,j}$ son los coeficientes
de $Tb_j$ expandido en la base $C$ de $W$. Y denotamos a $A$, por
$\mathcal{M}(T,B,C)$, o cuando no haya confusión con las bases,
$\mathcal{M}(T)$.

\subsubsection{Ejemplo}
Haremos notar, que encontrar la matriz correspondiente a un mapeo lineal
es un cuanto confuso, pues al expandir un vector del contradominio,
tenemos una suma donde los coeficientes de la columna se encuentran
en forma horizontal, sugiriendo erróneamente que corresponden a un
renglón.

Sea $T:F^3\rightarrow F^2$ dada con las bases estándar para el dominio y
contradominio por $T(x,y,z)^t = (x,y + z)^t$, tenemos:
{
$$T\begin{bmatrix}
    1\\
    0\\
    0    
\end{bmatrix}
 = 
 1\begin{bmatrix}
    1\\
    0
 \end{bmatrix}
 +0\begin{bmatrix}
    0\\
    1
 \end{bmatrix}
 $$
 $$T\begin{bmatrix}
    0\\
    1\\
    0
 \end{bmatrix}
 =
 0\begin{bmatrix}
    1\\
    0
 \end{bmatrix}
 +1\begin{bmatrix}
    0\\
    1
 \end{bmatrix}
 $$
 $$
 T\begin{bmatrix}
    0\\
    0\\
    1
 \end{bmatrix}
 =
 0\begin{bmatrix}
    1\\
    0
 \end{bmatrix}
 +
 1\begin{bmatrix}
    0\\
    1
 \end{bmatrix}
 $$
}
De manera que
$$\mathcal{M}(T) = \begin{bmatrix}
    1 & 0 & 0\\
    0 & 1 & 1
\end{bmatrix}$$

\subsection{Matriz de la multiplicación de mapeos lineales}
Sean $B$, $D$, y $C$ bases para $V$, $U$, y $W$, $T\in\mathcal{L}(V,U)$,
$S\in\mathcal{L}(U,W)$, entonces
$$\M(ST,B,C) = \M(S,D,C)\M(T,B,D)$$
\subsubsection*{Prueba}
Sean $v_i$, $u_k$ y $w_j$ elementos de las bases $B$, $D$, y $C$ respectivamente, $X=\M(S,D,C)$, y $Y=\M(T,B,D)$.

$\M(ST,B,C)$ está dada por: 
\begin{align*}
    STv_i &= S\left(\sum_k X_{i,k}u_k\right)\\
    &= \sum_k X_{i,k} Su_k\\
    &= \sum_k X_{i,k} \sum_j Y_{k,j} w_j \\
    &= \sum_j \left(\sum_k X_{i,k} Y_{k,j}\right) w_j
\end{align*}
Donde $\sum_k X_{i,k} Y_{k,j} = (\M(S,D,C)\M(T,B,D))_{i,j}$.
De aquí que la matriz de $ST$ es la matriz de $S$ por la matriz
de $T$ de acuerdo a sus respectivas bases.

\hfill ////

\subsection{El espacio de mapeos lineales y de matrices son isomorfos}
Sea $B=\{v_i\}_{i=1}^n$ una base para $V$ y $C=\{w_j\}_{j=1}^m$ una base
para $W$. Entonces $\mathcal{M}(\cdot, B, C)$ (que denotaremos por 
$\mathcal{M}$) es un isomorfismo
entre $\mathcal{L}(V,W)$, y $F^{m\times n}$.
\subsubsection*{Prueba}
Es trivial que $\M$ es lineal, solo falta probar que es inyectivo y 
sobreyectivo.

Sea $T\in\ker\M$, es decir $\M T$ es la matriz $0$. Luego por la
definición de $\M T$, $Tv_i = 0$. Por lo que en general $Tx=0$,
y $T=0$. Entonces $\dim\ker\M = 0$, y $\M$ es inyectivo.

Sea $A\in F^{m\times n}$, existe $T\in\mathcal{L}(V,W)$ definido
por $$Tv_j = \sum^m_{i=1}A_{i,j} w_j$$
De manera que $A=\M T$ y por lo tanto $\M$ es sobreyectivo.
\hfill ////

\subsubsection{Corolario: Dimensión del espacio de mapeos lineales}
$\mathcal{L}(V,W)$ con $V$ de dimensión $n$ y $W$ de dimensión $m$
ambas finitas, tiene dimensión $nm$ por virtud de que es isomorfo a
$F^{n\times m}$. Dicho independientemente de las bases
$$\dim \mathcal{L}(V,W) = \dim V \times \dim W$$

\subsection{La matriz de un vector}
Sea $\{v_i\}_{i=1}^n$ base de $V$ y $x=\sum_i\alpha_i v_i$, 
entonces se define la matriz de $x$ con esta base por la matriz
$n\times 1$ por
$$\M x = \begin{bmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha_n
\end{bmatrix}$$

\subsection{La columna de la matriz de una transformación}
Sea $\mathcal{L}(V,W)$ donde $v_1$ hasta $v_n$ son una base para $V$
y $w_1$ a $w_m$ una base para $W$, entonces
$$\M(T)_{.,j} = \M (Tv_j)$$
\subsubsection*{Prueba}
Sea $A=\M T$, tenemos
$$Tv_j = \sum^m_{i=1}A_{i,j} w_i$$
$$\M(Tv_j) = \begin{bmatrix}
    A_{1,j}\\
    \vdots\\
    A_{m,j}
\end{bmatrix}
=
A_{.,j}$$
\hfill ////

\subsection{Mapeos lineales actúan como matrices}
Sea $T$ un mapeo lineal con dominio de base $v_j$ y dimensión $n$,
contradominio de base $w_i$ y dimensión $m$, entonces
$$\M (Tx) = \M (T)\M (x)$$
\subsubsection*{Prueba}
Primero, sea $x$ expresado en la base $v_j$ con coeficientes $\alpha_j$,
entonces $$Tx =\sum_j\alpha_j Tv_j$$
luego $$\M(Tx)=\sum_j\alpha_j\M(Tv_j)$$
porque $\M$ es lineal. Como probamos antes
$M(Tv_i) = M(T)_{.,i}$, entonces
\begin{align*}
    \M(Tx) &= \sum_j\alpha_j\M(T)_{.,j}\\
    &= \M(T) \begin{bmatrix}
        \alpha_1 \\
        \vdots \\
        \alpha_n
    \end{bmatrix}\\
    &= \M(T)\M(x)
\end{align*}
Donde la segunda igualdad está dada porque la combinación
lineal de columnas de una matriz es la multiplicación de una matriz
por la columna con los coeficientes de la combinación lineal por 
elementos. \hfill ////

\subsubsection{Nota}
Recordemos que $\M(T)$ depende de las bases del dominio y 
contradominio de $T$. Mientras que $M(x)$ depende de la base
de $V$ donde está $x$.

\subsection{El rango de una matriz de un mapeo lineal}
Sea $T$ un mapeo lineal con dominio y contradominio finito,
entonces para cualquier selección de bases $\M(T)$ tiene rango
igual a $\dim\im T$.
\subsubsection*{Prueba}
El mapeo lineal $y \mapsto \M y$ con $y\in\im T$ es un isomorfismo
entre $\im T$ y el espacio de columnas de $\M(T)$, pues
$\M y=\M(Tx) = \M(T)\M(x)$ para algún $x\in V$. Y como ya mostramos,
tenemos que $\M(T)\M(x)$ está en el espacio de columnas de $\M(T)$.
Sigue que $\dim\im T$ es el rango de columnas de $\M(T)$.
\hfill ////

\subsection{La matriz identidad}
$I_n$ (o simplemente $I$ si no hay confusión) es la matriz identidad
de dimensiones $n\times n$ definida por $I_{i,j}=\delta_{i,j}$
donde $\delta_{i,j}$ es $0$ si $i\neq j$ y $1$ de otra manera.

\subsection{Inversa de una matriz}
Se dice que una matriz $A$ $m\times n$ es invertible si
existe una matriz $B$ $n\times m$ tal que $AB=I_m$ y
$BA=I_n$.

\subsubsection{Propiedades de una matriz invertible}
Análogamente a las transformaciones invertibles:
\begin{enumerate}
    \item Una matriz invertible es cuadrada.
    \item Su inversa es única y denotada por $A^{-1}$.
    \item Si $A$ y $B$ son invertibles de la misma dimensión,
    $AB$ es invertible y $(AB)^{-1}=B^{-1}A^{-1}$.
\end{enumerate}
\subsubsection*{Prueba}
1. Sea $A$ $m\times n$ invertible con $B$ como en la definición, y sean
$T= x\mapsto Ax$ y $S= y\mapsto By$ con $x\in F^n$ y $y\in F^m$.
Tenemos
\begin{align*}
    (TS)y &= ABy = y\\
    (ST)x &= BAx = x
\end{align*}
Entonces $S=T^{-1}$, de manera que $T$ es un isomorfismo entre
$F^n$ y $F^m$, por lo tanto $n=m$.

2. Como se tiene que $S=T^{-1}$ es único para $T$, $B=\M(S)$ también
es único para $A$.

3. Sean $S$ y $T$ como en 1, por la proposición de la inversa
de la multiplicación de mapeos lineales, sigue que
$(AB)^{-1} = B^{-1}A^{-1}$.
\hfill ////

\subsection{Matriz de la identidad en distintas bases}
Sean $B$ y $C$ distintas bases de $V$, entonces
$\M(I,B,C)$ es la inversa de\\
$\M(I,C,B)$.
\subsubsection*{Prueba}
Por como es la matriz de una multiplicación de transformaciones,
tomando en cuenta sus bases, tenemos
$$\M(I)=\M(II,C,C) = \M(I,B,C) \M(I,C,B)$$
y
$$\M(I)=\M(II,B,B) = \M(I,C,B) \M(I,B,C)$$
Donde la primer igualdad en cada ecuación viene de que la matriz
de la identidad de una base a la misma base es meramente la matriz
identidad. \hfill ////

\subsection{Matriz de cambio de bases}
Sea $T\in\mathcal{L}(V)$, y $v_i$, $u_i$ bases de $V$.
Entonces si $A=\M(T,v_i)$, $B=\M(T,u_i)$
entonces
$$A=\M(I,u_i, v_i)B\M(I,v_i,u_i)$$
\subsubsection*{Prueba}
Simplemente se factoriza $A$ usando que $T=TI$,
luego se factoriza el primer factor resultante $\M(T, u_i, v_i)$
usando que $T=IT$.
\begin{align*}
    A&=\M(TI,v_i)\\
    &=\M(T, u_i, v_i)\M(I, v_i, u_i)\\
    &=\M(IT, u_i, v_i)\M(I, v_i, u_i)\\
    &=\left(\M(I, u_i, v_i)\M(T,u_i)\right)\M(I, v_i, u_i)\\
    &=\M(I, u_i, v_i)B\M(I, v_i, u_i)
\end{align*}
\hfill ////

\subsection{La matriz de la inversa de un operador lineal}
Sea $T$ un operador lineal invertible en $V$, con base $v_i$ y
dimensión $n$. Entonces $\M(T^{-1}) = M(T)^{-1}$.
\subsubsection*{Prueba}
\begin{align*}
    \M(I)&=\M(TT^{-1})\\
    &=\M(T)\M(T^{-1})
\end{align*}
De donde multiplicando por $M(T)^{-1}$ por la izquierda,
directamente sigue el resultado.\hfill ////
\newpage

\subsection{Invertibilidad de los factores de un operador lineal}
Sean $S$ y $T$ operadores lineales sobre $V$ de dimensión finita,
entonces $ST$ es invertible ssi $S$ y $T$ son invertible.
\subsubsection*{Prueba}
Ya probamos $ST$ invertible $\impliedby$ $S$ y $T$ invertibles.

Ahora supongamos que $ST$ es invertible.

\textbf{$S$ es sobreyectiva:} Si $S$ no fuese sobreyectiva, $ST$ tampoco lo sería y tampoco sería
invertible. Por lo tanto $S$ es sobreyectiva.

\textbf{$T$ es inyectiva:} Si $T$ no fuese inyectiva, existen $x\neq y$ tal que $Tx=Ty$,
luego $STx=STy$, y $ST$ no sería inyectiva ni invertible. Por lo
tanto $T$ es inyectiva.

Como el espacio $V$ es de dimensión finita y los mapeos lineales son 
operadores, sobreyectividad o inyectividad implican invertibilidad, así 
$S$ y $T$ son invertibles.
\hfill ////
\newpage

\section{Espacios cocientes}
\subsection{Traslación de un subconjunto}
Definimos la traslación de un conjunto $U\subset V$ por $x\in V$ como
el conjunto $x+U:=\{x+u: u\in U\}$.

\subsection{Definición de espacio cociente}
Si $U$ es un subespacio de $V$, definimos el espacio cociente
$V/U$ como el conjunto de todas las traslaciones de $U$:
$$V/U=\{x+U:x\in V\}$$

\subsection{Una traslación es igual a otra o ajenos}\label{traslaciones}
Sean $v$ y $w$ en $V$, y $U$ un subespacio de $V$. Entonces
$$v-w\in U \iff v+U = w+V \iff (v+U)\cap(w+U)\neq\emptyset$$
\subsubsection*{Prueba}
La demostración es un ejercicio elemental de álgebra y conjuntos.

De aquí en adelante $U$ es un subespacio de $V$, y los elementos en
minúsculas son elementos cualesquiera de $V$
\subsection{Equivalencia}
Como acabamos de demostrar $v-w\in U \iff v+U=w+U$,
así que definimos la relación de equivalencia $\equiv$ por
$v\equiv w \iff v-w\in U$.

Queremos formar un espacio vectorial sobre el espacio cociente.
Para esto tenemos que definir correctamente la suma y multiplicación 
por escalar.

\subsection{Suma}
Sean $v\equiv x$ y $w\equiv y$, entonces $v+w \equiv x+y$.
\subsubsection*{Prueba}
Por definición tenemos $v-x\in U$ y $w-y\in U$.
Luego
\begin{align*}
    (v+w) - (x+y) &= (v-x) + (w-y) \in U
\end{align*} 
por cerradura del subespacio.\hfill ////

Esto prueba que si se suman vectores distintos, se pueden sumar
cualquiera otros vectores equivalentes y obtener una suma equivalente a la otra.

De manera que definimos $(x+U)+(y+U) := (x+y)+U$ para \\
cualquier $x,y$.


\subsection{Multiplicación escalar}
Sea $v\equiv x$ y $\lambda$ un escalar, entonces
$\lambda v\equiv \lambda x$.
\subsubsection*{Prueba}
$v-x\in U$ entonces
$$\lambda v -\lambda x = \lambda(v-x) \in U$$
por cerradura de $U$. \hfill ////

Así que definimos $\lambda (v + U):= \lambda v + U$.

Ambas operaciones hacen que $V/U$ sea un espacio vectorial.
Hacemos notar que $0+U$ es $0$ en este espacio vectorial.

\subsection{Mapeo cociente}
El mapeo cociente $\pi: V \rightarrow V/U$ está definido por
$$\pi(x)=x+U$$
y es lineal.

\subsection{Dimensión del espacio cociente}
Si $V$ es de dimensión finita,
$$\dim V/U = \dim V - \dim U$$
\subsubsection*{Prueba}
Tenemos que $x\in\ker \pi \iff x+U = 0+U \iff x\in U$, donde la
segunda equivalencia está dada por \ref{traslaciones}. De
manera que $\ker\pi = U$. Como $\pi: V \rightarrow V/U$ y $V$ es de dimensión finita, por el teorema fundamental de mapeos lineales
$$\dim V = \dim\ker\pi + \dim\im\pi$$
como $\im\pi=V/U$ y $\ker\pi = U$ sigue el resultado esperado
$$\dim V/U = \dim V - \dim U$$
\hfill ////

Para la siguiente definición hacemos notar que para
$T\in\mathcal{L}(V,W)$ si $x+\ker T = y+\ker T$, por \ref{traslaciones}
tenemos $x-y\in\ker T$ y por lo tanto $Tx=Ty$.
\subsection{Mapeo inducido por el kernel de un mapeo lineal}
Dado $T$ definimos $\widetilde T: V/\ker T\rightarrow W$ por
$$\widetilde T(x+\ker T) = Tx$$
y notamos que $\widetilde T$ es lineal.

Una propiedad se hace inmediatamente aparente.
$$x\in\ker T \iff x+\ker T\in \ker\widetilde{T}$$
La suficiencia viene de \ref{traslaciones}.
La necesidad, porque si $x+\ker T\in \ker\widetilde{T}$, entonces
$$\widetilde T(x+\ker T) = Tx = 0$$

De la necesidad tenemos que $x+\ker T\in \ker\widetilde{T}$
implica que $x\in\ker T$, es decir $x +\ker T = 0+\ker T$. De manera
que $\ker\widetilde{T} = \{0+\ker T\}$, y $\widetilde{T}$ es inyectiva.

\subsubsection{Más propiedades de este mapeo}
Sea $T\in\mathcal{L}(V,W)$ entonces
\begin{enumerate}
    \item $\widetilde{T}\circ\pi = T$, donde
    $\pi:V/\ker T\rightarrow W$ es el mapeo cociente.
    \item $\widetilde{T}$ es inyectiva.
    \item $\im \widetilde{T} = \im T$.
\end{enumerate}
\subsubsection*{Prueba}
\begin{enumerate}
    \item $(\widetilde{T}\circ\pi)x = \widetilde{T}(x+\ker T) = Tx$
    \item Lo probamos anteriormente.
    \item Por definición.
\end{enumerate}
\hfill ////

Por consecuencia de punto 2 y 3, $V/\ker T$ y $\im T$ son
isomorfos.
\newpage

\section{Dualidad}

\newpage
\section*{Referencias}
\begin{enumerate}
    \item Linear Algebra Done Right 4ta edición, por Axler, Sheldon.
\end{enumerate}
\end{document}
